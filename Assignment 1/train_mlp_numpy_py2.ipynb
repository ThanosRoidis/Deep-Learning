{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module implements training and evaluation of a multi-layer perceptron in NumPy.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "from mlp_numpy import MLP\n",
    "import cifar10_utils_py2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Default constants\n",
    "LEARNING_RATE_DEFAULT = 2e-3\n",
    "WEIGHT_REGULARIZER_STRENGTH_DEFAULT = 0.\n",
    "WEIGHT_INITIALIZATION_SCALE_DEFAULT = 1e-4\n",
    "BATCH_SIZE_DEFAULT = 200\n",
    "MAX_STEPS_DEFAULT = 1500\n",
    "DNN_HIDDEN_UNITS_DEFAULT = '100'\n",
    "\n",
    "# Directory in which cifar data is saved\n",
    "DATA_DIR_DEFAULT = './cifar10/cifar-10-batches-py'\n",
    "LOG_DIR = './logs/cifar10/mlp_numpy/'\n",
    "MODEL_FOLDER = ''\n",
    "\n",
    "FLAGS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_init_scale : 0.0001\n",
      "data_dir : ./cifar10/cifar-10-batches-py\n",
      "learning_rate : 0.002\n",
      "batch_size : 200\n",
      "weight_reg_strength : 0.0\n",
      "dnn_hidden_units : 100\n",
      "max_steps : 1500\n",
      "step 0: loss: 2.302503, 2.302503, acc: 0.143100\n",
      "step 100: loss: 1.759325, 1.759325, acc: 0.367300\n",
      "step 200: loss: 1.650975, 1.650975, acc: 0.418200\n",
      "step 300: loss: 1.675326, 1.675326, acc: 0.408100\n",
      "step 400: loss: 1.586292, 1.586292, acc: 0.441700\n",
      "step 500: loss: 1.631646, 1.631646, acc: 0.424000\n",
      "step 600: loss: 1.604198, 1.604198, acc: 0.424800\n",
      "step 700: loss: 1.740624, 1.740624, acc: 0.408500\n",
      "step 800: loss: 1.619057, 1.619057, acc: 0.433100\n",
      "step 900: loss: 1.486658, 1.486658, acc: 0.472800\n",
      "step 1000: loss: 1.517048, 1.517048, acc: 0.464500\n",
      "step 1100: loss: 1.509000, 1.509000, acc: 0.476000\n",
      "step 1200: loss: 1.641798, 1.641798, acc: 0.438100\n",
      "step 1300: loss: 1.566983, 1.566983, acc: 0.452300\n",
      "step 1400: loss: 1.494416, 1.494416, acc: 0.473600\n",
      "test: 1.75098499441 0.4099\n",
      "results saved on: ./logs/cifar10/mlp_numpy/mlp_numpy_20171111-1506/\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    \"\"\"\n",
    "    Performs training and evaluation of MLP model. Evaluate your model on the whole test set each 100 iterations.\n",
    "    \"\"\"\n",
    "    ### DO NOT CHANGE SEEDS!\n",
    "    # Set the random seeds for reproducibility\n",
    "    np.random.seed(42)\n",
    " \n",
    "\n",
    "    # FLAGS.dnn_hidden_units = '100'\n",
    "    ## Prepare all functions\n",
    "    # Get number of units in each hidden layer specified in the string such as 100,100\n",
    "    if FLAGS.dnn_hidden_units:\n",
    "        dnn_hidden_units = FLAGS.dnn_hidden_units.split(\",\")\n",
    "        dnn_hidden_units = [int(dnn_hidden_unit_) for dnn_hidden_unit_ in dnn_hidden_units]\n",
    "    else:\n",
    "        dnn_hidden_units = []\n",
    "\n",
    "    train_cifar = True\n",
    "\n",
    "    if train_cifar:\n",
    "        dataset = cifar10_utils_py2.get_cifar10(FLAGS.data_dir)\n",
    "        n_input = 3072\n",
    "        n_classes = 10\n",
    "        norm_const = 1\n",
    "    else:\n",
    "        dataset = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "        n_input = 784\n",
    "        n_classes = 10\n",
    "        norm_const = 1\n",
    "        \n",
    "    #SAVE FLAGS TO A FILE\n",
    "    MODEL_FOLDER = LOG_DIR + 'mlp_numpy_' + time.strftime(\"%Y%m%d-%H%M\") + '/'\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(os.path.dirname(MODEL_FOLDER)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(MODEL_FOLDER))\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "    file = open(MODEL_FOLDER + 'flags.txt', 'w+')\n",
    "    for key, value in vars(FLAGS).items():\n",
    "        file.write(key + ' : ' + str(value) + '\\n')\n",
    "    file.close()\n",
    "    \n",
    "    test_results = []\n",
    "\n",
    "\n",
    "#   FLAGS.weight_reg_strength = 0.001\n",
    "#   FLAGS.learning_rate = 0.1\n",
    "  # FLAGS.max_steps = 10000\n",
    "\n",
    "    mlp = MLP(n_input, dnn_hidden_units, n_classes,\n",
    "            weight_decay=FLAGS.weight_reg_strength,\n",
    "            weight_scale=FLAGS.weight_init_scale)\n",
    "\n",
    "    np.set_printoptions(threshold= np.nan)\n",
    "\n",
    "    for step in range(FLAGS.max_steps):\n",
    "        x, y = dataset.train.next_batch(FLAGS.batch_size)\n",
    "        x = np.reshape(x, (-1, n_input))  / norm_const\n",
    "\n",
    "        logits = mlp.inference(x)\n",
    "\n",
    "\n",
    "        loss, full_loss = mlp.loss(logits, y)\n",
    "        mlp.train_step(full_loss, FLAGS)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            x = dataset.test.images\n",
    "            x = np.reshape(x, (-1, n_input)) / norm_const\n",
    "            y = dataset.test.labels\n",
    "\n",
    "            logits = mlp.inference(x)\n",
    "            loss, full_loss = mlp.loss(logits, y)\n",
    "            acc =  mlp.accuracy(logits, y)\n",
    "            print('step %d: loss: %f, %f, acc: %f' % (step, loss, full_loss,acc))\n",
    "            \n",
    "            test_results.append((step, full_loss, acc)) \n",
    "\n",
    "    #Evaluate on test set after the training has finished\n",
    "    x = dataset.test.images / norm_const\n",
    "    x = np.reshape(x, (-1, n_input))\n",
    "    y = dataset.test.labels\n",
    "\n",
    "    logits = mlp.inference(x)\n",
    "    L, _ = mlp.loss(logits, y)\n",
    "    print('test:', L, mlp.accuracy(logits, y))\n",
    "    \n",
    "    test_results.append((FLAGS.max_steps, full_loss, acc))  \n",
    "    \n",
    "    #Save results to file\n",
    "    with open(MODEL_FOLDER + 'results.pkl', 'wb') as fp:\n",
    "        pickle.dump(test_results, fp)\n",
    "        \n",
    "    print('results saved on: ' + MODEL_FOLDER)\n",
    "\n",
    "\n",
    "\n",
    "def print_flags():\n",
    "  \"\"\"\n",
    "  Prints all entries in FLAGS variable.\n",
    "  \"\"\"\n",
    "  for key, value in vars(FLAGS).items():\n",
    "    print(key + ' : ' + str(value))\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function\n",
    "    \"\"\"\n",
    "    # Print all Flags to confirm parameter settings\n",
    "    print_flags()\n",
    "\n",
    "    if not os.path.exists(FLAGS.data_dir):\n",
    "        os.makedirs(FLAGS.data_dir)\n",
    "\n",
    "    # Run the training operation\n",
    "\n",
    "    # for layer in range(5, 0, -1):\n",
    "    #   print(layer)\n",
    "    # return\n",
    "    train()\n",
    "\n",
    "  \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dnn_hidden_units', type = str, default = DNN_HIDDEN_UNITS_DEFAULT,\n",
    "                  help='Comma separated list of number of units in each hidden layer')\n",
    "parser.add_argument('--learning_rate', type = float, default = LEARNING_RATE_DEFAULT,\n",
    "                  help='Learning rate')\n",
    "parser.add_argument('--max_steps', type = int, default = MAX_STEPS_DEFAULT,\n",
    "                  help='Number of steps to run trainer.')\n",
    "parser.add_argument('--batch_size', type = int, default = BATCH_SIZE_DEFAULT,\n",
    "                  help='Batch size to run trainer.')\n",
    "parser.add_argument('--weight_init_scale', type = float, default = WEIGHT_INITIALIZATION_SCALE_DEFAULT,\n",
    "                  help='Weight initialization scale (e.g. std of a Gaussian).')\n",
    "parser.add_argument('--weight_reg_strength', type = float, default = WEIGHT_REGULARIZER_STRENGTH_DEFAULT,\n",
    "                  help='Regularizer strength for weights of fully-connected layers.')\n",
    "parser.add_argument('--data_dir', type = str, default = DATA_DIR_DEFAULT,\n",
    "                  help='Directory for storing input data')\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (MODEL_FOLDER + 'results.pkl', 'rb') as fp:\n",
    "    itemlist = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2.3025031447540418, 0.1431),\n",
       " (100, 1.7593248583641168, 0.36730000000000002),\n",
       " (200, 1.6509751106431194, 0.41820000000000002),\n",
       " (300, 1.675326062394713, 0.40810000000000002),\n",
       " (400, 1.5862916592989746, 0.44169999999999998),\n",
       " (500, 1.6316462978692856, 0.42399999999999999),\n",
       " (600, 1.6041976861938014, 0.42480000000000001),\n",
       " (700, 1.7406241789718035, 0.40849999999999997),\n",
       " (800, 1.6190573983364365, 0.43309999999999998),\n",
       " (900, 1.4866582967328159, 0.4728),\n",
       " (1000, 1.5170482235567944, 0.46450000000000002),\n",
       " (1100, 1.5089998029740581, 0.47599999999999998),\n",
       " (1200, 1.6417978476869572, 0.43809999999999999),\n",
       " (1300, 1.5669825193082445, 0.45229999999999998),\n",
       " (1400, 1.4944159611865293, 0.47360000000000002),\n",
       " (1500, 1.716714959303024, 0.47360000000000002)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
